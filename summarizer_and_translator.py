# -*- coding: utf-8 -*-
"""Summarizer and translator

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sojJvxrZSviImUrWTP3FX7o5TL54WQ5i
"""

!pip install -q transformers sentencepiece newspaper3k

import newspaper
from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
import datetime

CNN_News = newspaper.build('https://edition.cnn.com/', language='en', memoize_articles=False)

# Preform scrapping on this month's articles only (Top 10 articles for example)

# Get today's date
today = datetime.datetime.now()

articles=[]

# Loop over all the articles
for article in CNN_News.articles:
  article.download()
  article.parse()
  if (article.publish_date != None) and (len(article.text)>0) and (article.publish_date.month == today.month) and (article.publish_date.year == today.year) :
    # Adding the article, its publications date, and its url to our list of articles 
    articles.append([article.text, article.publish_date, article.url])

  # Get only the first 10 articles
  if len (articles) == 10 :
    break

# Text summarization using bart-large-cnn model from huggingface

# loading the pretrained model
model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
 
for article in articles:
  # Passing the article's original text 
  inputs = tokenizer([article[0][:512]], return_tensors='pt')
  # Generate Summary
  summary_ids = model.generate(inputs['input_ids'] , early_stopping=True)
  summary = [tokenizer.decode(summary_id, skip_special_tokens=True, clean_up_tokenization_spaces=False) for summary_id in summary_ids][0]
  # Adding the summary to the list of each article
  article.append(summary)

# Machine Translation for each article using mbart-large-50-many-to-many-mmt from huggingface

# loading the pretrained model
model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

for article in articles :
  tokenizer.src_lang = "en_XX"
  tokenizer.dst_lang = "ar_AR"
  # Passing the artical's text summary
  encoded_ar = tokenizer(article[3][:512], return_tensors="pt")
  # Generate translation
  generated_tokens = model.generate(**encoded_ar, forced_bos_token_id=tokenizer.lang_code_to_id["ar_AR"])
  translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
  article.append(translation)

print("The article is :", articles[7][0])
print("The article url is :", articles[7][2])
print("The article summary is :", articles[7][3])
print("The article summary translation is :",articles[7][4])